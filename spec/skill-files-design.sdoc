# Lexica — Knowledge Files for AI Agents @lexica-design
{
    # Meta @meta
    {
        author: Michael + Claude
        date: 2026-02-19
        status: Draft — Refined after red team, scoping, automation, discovery model, and single-directory convention
    }

    # Problem Statement @problem
    {
        AI agents need context to be useful, but context windows are expensive, slow to fill,
        and noisy when overfilled. The current state of the art — dump a big AGENTS.md at the
        root, rely on RAG to guess what's relevant, or hand-curate tool descriptions — is crude.

        Documentation and skill knowledge naturally form a tree. When an agent is working at
        some node in that tree, it should be able to navigate to that node learning only what
        it needs, without polluting its context with irrelevant material. But it must still
        know what information and skills are *available* should they be needed.

        Some knowledge is universal (language patterns, general tooling). Some is repo-specific
        (project architecture, coding conventions). Some is local to a subtree (module-specific
        patterns, API details for one subsystem). An agent needs the right knowledge for where
        it is working, drawn from all three levels, without loading knowledge meant for other
        parts of the tree.

        This is the progressive disclosure problem applied to AI agent context management.
    }

    # Core Insight @core-insight
    {
        Design a knowledge system that agents can *navigate deliberately* rather than having
        knowledge pushed at them via RAG or monolithic context files. This mirrors how a human
        expert works — they know the map of what topics exist and where to find them, and they
        selectively dive deep only where needed.

        The key idea: structure each knowledge file with well-identified sections, then let
        tooling handle everything else — identity, discovery metadata, indexing, and extraction.
        The authoring burden should be as close to "just write the content" as possible, with
        AI and tooling generating all structural metadata.

        Knowledge is scoped to where it lives in the filesystem tree. An agent's visible
        knowledge set is determined by its working directory — it sees knowledge from its
        location up to the repo root, plus any global shared knowledge. Closer knowledge
        ranks higher. This mirrors how configuration cascades work (gitignore, tsconfig,
        sdoc.config.json) and requires no explicit wiring.

        We call this system **Lexica** — a structured approach to organising, discovering,
        and navigating project knowledge for both AI agents and human developers.
    }

    # Guiding Principle: Author Content, Generate Everything Else @author-content
    {
        Every layer of required metadata is a friction point that degrades adoption. History
        shows that systems requiring authors to maintain metadata separately from content
        produce stale metadata (DITA, Javadoc, Dublin Core).

        The radical version of "author little, generate much" is: **the author's only job is
        to write good content with clear section headings.** Everything else is generated by
        tooling or AI agents — before commit, at merge time, or on the fly.

        {[table]
            Thing | Who creates it | When | How
            UUID | Tooling | File creation | VS Code command, pre-commit hook, or CI generates it automatically
            \@about | AI agent | Authoring or merge time | AI reads content sections and generates 2-5 line discovery paragraph
            Section IDs for extraction | Tooling | On the fly | Derived from heading text (slugified) by the MCP server or parser
            Explicit \@id | Author (optional) | When writing | Only needed when the author wants a stable cross-reference point
            Tags | AI agent (optional) | Merge time | AI suggests tags from content; team can review or ignore
            Manifest | Tooling | On demand | Script walks the tree, emits JSON
        }

        # What the Author Actually Does @author-workflow
        {
            {[#]
                1. Create a new knowledge file (VS Code command generates the skeleton with UUID)
                2. Write content with clear section headings
                3. Optionally add explicit \@ids to sections they want as stable cross-reference
                   targets
                4. Commit
            }

            Everything else happens automatically:

            {[.]
                - A pre-commit hook or CI step generates \@about if missing, by asking an AI
                  agent to summarise the content in 2-5 lines
                - The same step can generate tags if the team wants them
                - The merge gate validates structure (UUID exists, \@about exists)
                - The manifest regenerates to include the new file
            }
        }

        # The \@about Paragraph @about-generation
        {
            The \@about section is the single most important piece of metadata for discovery.
            It answers: "Is this file relevant to my current task?"

            It should cover:
            {[.]
                - What topics this file addresses
                - When an agent or developer would need it
                - Any prerequisites or assumed knowledge
            }

            2-5 lines is the target length.

            **Generation strategy:** An AI agent reads the content sections and produces the
            \@about paragraph. This can happen at several points:

            {[.]
                - **At authoring time** — a VS Code command ("SDOC: Generate About") reads the
                  content and fills in \@about. The author reviews and can tweak.
                - **At commit time** — a pre-commit hook generates or updates \@about if the
                  content has changed. The author sees it in the diff and can amend.
                - **At merge time** — CI generates \@about as part of the merge gate. The PR
                  shows the generated metadata for review.
                - **On the fly** — the MCP server generates \@about for files that lack it,
                  caching the result. Zero author involvement, but less controlled.
            }

            The authored \@about in the file is the "blessed" version. If present, tooling
            uses it as-is. If missing, tooling generates one. This means a knowledge file with
            no metadata at all is still functional — it just gets auto-generated discovery
            text.

            An AI-generated \@about will typically be *more consistent* than a human-written
            one, because it follows the same pattern every time. The risk of a bad summary
            (which degrades discovery) is lower than the risk of no summary at all.
        }

        # Section IDs: Explicit vs Derived @section-ids
        {
            Section IDs serve two different purposes:

            {[#]
                1. **Cross-referencing** between documents (e.g., "see \@smart-ptrs") — this
                   requires a meaningful, stable, human-chosen \@id
                2. **Tool extraction** (e.g., "get me the Smart Pointers section") — this just
                   needs any identifier that addresses a section
            }

            For purpose 2, tooling derives IDs from heading text automatically. "Smart
            Pointers" becomes \`smart-pointers\` (slugified). "RAII Pattern" becomes
            \`raii-pattern\`. These derived IDs are used by \`get_sections\` and \`get_section\`
            tools but are never written to the file.

            When a heading has an explicit \@id, that takes precedence over the derived ID.
            This means:

            {[.]
                - An author who just writes content with clear headings gets working section
                  extraction for free — no \@ids needed
                - An author who wants stable cross-reference targets adds explicit \@ids to
                  those sections
                - Renaming a heading does not break tool extraction (derived ID updates
                  automatically) but does break cross-references to the derived ID (which
                  is why important reference targets should use explicit \@ids)
            }
        }
    }

    # Why SDOC @why-sdoc
    {
        SDOC is not the only supported format — discovery is format-agnostic and Markdown
        files work too. But SDOC provides the richest tooling for progressive disclosure,
        and several properties of the format make it the best substrate for machine-navigable
        knowledge files.

        # Unambiguous Section Extraction @why-braces
        {
            In Markdown, "extract the Smart Pointers section" requires guessing where the
            section ends — at the next heading of equal depth? Lesser depth? What if heading
            levels are inconsistent? The answer depends on conventions that vary between
            authors and documents.

            In SDOC, section boundaries are explicit: everything between \`{\` and the
            matching \`}\`. The parser produces an unambiguous AST. A tool that needs to
            return exactly one section on demand gets it right every time, with no
            heuristics.

            This is the foundation of the \`get_section\` tool — and it only works reliably
            with explicit scoping.
        }

        # Built-In Metadata Support @why-meta
        {
            The \@meta scope already exists in SDOC for per-file configuration (style,
            header, footer) with key:value syntax. Extending it for UUID and tags is
            natural — no new syntax needed. The parser already extracts \@meta and
            separates it from rendered content.

            In Markdown, metadata requires frontmatter YAML — a different syntax embedded
            in the document, parsed by different tools, with different escaping rules. In
            SDOC, metadata is just another scope in the same format.
        }

        # The \@id System @why-ids
        {
            SDOC headings support \@id natively: \`# Smart Pointers \@smart-ptrs\`. These
            IDs are part of the parsed AST and are used for cross-referencing. The skill
            file design uses the same mechanism for section extraction — no new concept
            for authors to learn.

            IDs are stable even when heading text changes ("Smart Pointers" could be
            renamed to "Pointer Types" without breaking \@smart-ptrs references).
        }

        # Hierarchical Config Pattern Already Proven @why-config-chain
        {
            The SDOC extension already implements hierarchical \`sdoc.config.json\` — config
            files at each directory level are merged from root down to the file being
            rendered. The closest config wins for any given key.

            The Lexica scoping model (see \@scoping) follows the same pattern: \`lexica/\`
            directories at each level, closest knowledge ranked highest. This is a
            pattern developers already understand.
        }

        # AI Agents Can Author SDOC @why-ai-authoring
        {
            The format is simple enough that AI agents can write correct knowledge files.
            The brace structure is unambiguous to produce (unlike Markdown where heading
            levels must be tracked carefully). A template plus "fill in the content
            sections" is a straightforward prompt.

            This matters because in the Lexica model, AI agents are not just consumers
            of knowledge files — they are also producers. An agent can create a new file,
            generate its \@about, and even update existing files when knowledge changes.
            The format being AI-friendly in both directions (reading and writing) is
            essential.
        }

        # Existing Tooling @why-tooling
        {
            The SDOC parser, renderer, VS Code extension (with live preview and
            formatting), and document server already exist. Knowledge files are immediately
            previewable, formattable, and browsable with no new tools. The implementation
            path only requires adding extraction and manifest functions to the existing
            parser — not building a new tool chain.
        }
    }

    # File Structure @structure
    {
        A knowledge file is an SDOC document. At minimum, it is just content with headings.
        Metadata (\@meta, \@about) can be added by the author or generated by tooling.

        # Minimal File (Author Writes Only Content) @structure-minimal
        {
            ```sdoc
            # C++ Memory Management
            {
                # RAII Pattern
                {
                    Resource Acquisition Is Initialization (RAII) binds resource
                    lifetime to object lifetime...
                }

                # Smart Pointers
                {
                    Modern C++ provides three smart pointer types...
                }

                # Custom Allocators
                {
                    For performance-critical paths, custom allocators can...
                }
            }
            ```

            Tooling will:
            {[.]
                - Generate a UUID and inject it into \@meta
                - Generate an \@about paragraph from the content
                - Derive section IDs from headings: \`raii-pattern\`, \`smart-pointers\`,
                  \`custom-allocators\`
            }
        }

        # Full File (After Tooling Has Run) @structure-full
        {
            ```sdoc
            # C++ Memory Management @cpp-memory
            {
                # Meta @meta
                {
                    uuid: 550e8400-e29b-41d4-a716-446655440000
                    type: skill
                    tags: cpp, memory, raii, smart-pointers
                }

                # About @about
                {
                    Memory management patterns in modern C++. Covers RAII, smart
                    pointer selection (unique, shared, weak), custom allocators
                    for performance-critical paths, and common memory bugs.
                    Assumes familiarity with basic C++ object lifecycle.
                }

                # RAII Pattern @raii
                {
                    Resource Acquisition Is Initialization (RAII) binds resource
                    lifetime to object lifetime...
                }

                # Smart Pointers @smart-ptrs
                {
                    Modern C++ provides three smart pointer types...
                }

                # Custom Allocators @allocators
                {
                    For performance-critical paths, custom allocators can...
                }
            }
            ```

            This is the same file after:
            {[.]
                - Tooling generated UUID and \@meta
                - AI generated \@about and tags
                - Author added explicit \@ids to sections they want as cross-reference targets
                  (optional — if absent, derived IDs still work for extraction)
            }
        }
    }

    # Directory Structure @directories
    {
        Lexica uses a single directory name to organise knowledge: \`lexica/\`. This
        directory is plain, visible, and format-agnostic. It can appear at any level
        of the filesystem tree.

        # One Directory, Two Types @one-dir
        {
            All knowledge files live in \`lexica/\` directories. The \`type\` field in
            \@meta distinguishes the two fundamental kinds of knowledge:

            {[table]
                Type | Contains | Character | Examples
                \`type: skill\` | How to do things | Prescriptive — patterns, techniques, recipes, conventions | C++ memory patterns, OAuth integration guide, coding conventions, deployment runbook
                \`type: doc\` | What things are, what's been decided, where things stand | Descriptive — specs, designs, plans, status, architecture | API contract, database schema, project roadmap, architecture decisions, sprint status
            }

            The type describes *content*, not *audience*. Both types are useful to
            humans and agents alike. There is no \`agents/\` wrapper — knowledge is not
            hidden behind an audience label.
        }

        # Why These Two Types @why-two-types
        {
            The distinction maps to two fundamental questions an agent (or developer) asks:

            {[.]
                - **"How do I do this?"** — look for \`type: skill\` files
                - **"What is this? What's the plan? What was decided?"** — look for \`type: doc\` files
            }

            Everything fits into one of these. Conventions and guardrails are skills (how to
            work). API contracts and schemas are docs (what things are). Planning documents
            and status updates are docs (where things stand). Runbooks and playbooks are
            skills (how to perform a task). If something does not fit neatly, it is probably
            a doc — descriptive is the broader category.
        }

        # Scope Levels @scope-levels
        {
            {[table]
                Level | Location | What lives here | Example
                Global | \`~/lexica/\` | Language patterns, general tooling, universal knowledge | C++ idioms, git workflows, design patterns
                Repo | \`repo/lexica/\` | Project architecture, conventions, system-wide specs and plans | Architecture, coding conventions, API contracts, project roadmap
                Directory | \`any/dir/lexica/\` | Module-specific knowledge | Auth patterns, auth API spec, GPU pipeline, rendering design notes
            }

            Example layout for a C++ project:

            ```
            ~/lexica/                           # Global knowledge (shared across all repos)
                cpp-memory.sdoc                 # type: skill
                cpp-concurrency.sdoc            # type: skill
                design-patterns.sdoc            # type: skill
                git-workflows.sdoc              # type: skill

            repo/
                lexica/                         # Repo-wide knowledge
                    coding-conventions.sdoc     # type: skill
                    error-handling.sdoc         # type: skill
                    deployment.sdoc             # type: skill
                    project-architecture.sdoc   # type: doc
                    api-design-contract.sdoc    # type: doc
                    database-schema.sdoc        # type: doc
                    roadmap.sdoc                # type: doc
                    status.sdoc                 # type: doc
                src/
                    lexica/                     # src subtree knowledge
                        build-system.sdoc       # type: skill
                    auth/
                        lexica/                 # auth module knowledge
                            auth-patterns.sdoc          # type: skill
                            oauth-integration.sdoc      # type: skill
                            auth-api.sdoc               # type: doc
                            session-lifecycle.sdoc       # type: doc
                            token-format.sdoc           # type: doc
                        login.cpp
                        session.cpp
                    rendering/
                        lexica/                 # rendering module knowledge
                            gpu-pipeline.sdoc           # type: skill
                            shader-compilation.sdoc     # type: skill
                            frame-timing.sdoc           # type: doc
                            render-pass-order.sdoc      # type: doc
                        renderer.cpp
            ```

            Knowledge files are visible, browsable, and obvious. A developer listing a
            directory sees \`lexica/\` and knows it contains project knowledge. The \`type\`
            field in each file's \@meta distinguishes skills from docs. No hidden
            directories, no format lock-in.
        }

        # Strict Directory Convention @strict-convention
        {
            Knowledge files live in \`lexica/\` directories. Period. No embedding knowledge
            files alongside code, no knowledge files outside of \`lexica/\` directories.

            This is a deliberate constraint. The benefit of a strict convention:

            {[.]
                - **Discovery is trivial.** Tooling walks up from the working directory and
                  checks for \`lexica/\` at each level. Deterministic, fast, no scanning.
                - **Predictable structure.** Everyone knows where to find knowledge and where
                  to put it.
                - **Clean code directories.** Code is code, knowledge is in its named folder.
                  One directory away, not across the repo.
                - **Submodules work cleanly.** A shared knowledge repo is just a submodule at
                  \`lexica/shared/\`.
                - **No name collisions.** The name \`lexica/\` is purpose-specific and does not
                  collide with existing conventions like \`docs/\` (often used for GitHub Pages
                  or user-facing documentation).
            }

            The objection "creating a directory for one file is overhead" is weak — \`mkdir\`
            is cheap and consistency is valuable. Knowledge close to the code it describes
            (e.g., \`src/auth/lexica/session-lifecycle.sdoc\`) is one directory away, not across
            the repo.
        }
    }

    # Knowledge Discovery and Resolution @scoping
    {
        Knowledge is discovered by convention: files in \`lexica/\` directories at any
        level of the tree. The directory name is plain and visible. The discovery
        mechanism is format-agnostic — files can be \`.sdoc\`, \`.md\`, or any other format
        the tooling supports.

        An agent's visible knowledge set is determined by its working directory — it sees
        everything from its location up to the repo root, plus global shared knowledge.

        # Resolution Rules @resolution-rules
        {
            When an agent working at \`repo/src/auth/\` asks "what knowledge is available?",
            the tooling walks upward collecting \`lexica/\` directories:

            {[#]
                1. \`repo/src/auth/lexica/\` — most local (highest priority)
                2. \`repo/src/lexica/\` — parent directory
                3. \`repo/lexica/\` — repo-wide
                4. \`~/lexica/\` — global shared knowledge (lowest priority)
            }

            Key rules:

            {[.]
                - **No shadowing.** All knowledge at all levels is visible. A global C++ skill
                  and a repo-specific C++ skill both appear in results. The agent decides
                  which to read.
                - **Proximity ranking.** Closer knowledge ranks higher in search results. When
                  an agent in \`src/auth/\` searches for "authentication," the auth-level skill
                  appears before a repo-level skill on the same topic.
                - **Walk stops at repo root.** The upward walk collects \`lexica/\` from each
                  directory up to and including the repo root (detected by \`.git/\`). Global
                  knowledge from \`~/lexica/\` is appended separately.
                - **Scope metadata in results.** Each file in the manifest carries a \`scope\`
                  field ("global", "repo", or the relative directory path) and a \`type\` field
                  ("skill" or "doc") so the agent understands where the knowledge comes from,
                  how specific it is, and whether it is prescriptive or descriptive.
                - **Format-agnostic.** The tooling scans for all supported file types in
                  \`lexica/\` directories, not just \`.sdoc\` files. SDOC files get the best
                  progressive disclosure (unambiguous section extraction), but a Markdown
                  file still gets discovered and served.
            }

            This mirrors how configuration cascading already works in the SDOC extension
            (\`sdoc.config.json\` chains from root to file directory) and in tools like
            \`.gitignore\`, \`tsconfig.json\`, and \`.eslintrc\`.
        }

        # Shared Knowledge Distribution @shared-knowledge
        {
            Knowledge common across many repos (language patterns, tool usage, general design)
            belongs at the global level or in a shared repository.

            # Recommended: Git Submodule @submodule-distribution
            {
                The recommended approach is a shared knowledge repo mounted as a git submodule
                at \`repo/lexica/shared/\`. This gives you the full git workflow for
                knowledge authoring — branches, PRs, code review, merge conflict resolution —
                and clean distribution to consuming repos.

                The usual complaints about submodules do not apply here:

                {[.]
                    - **Read-only from the consumer's perspective.** Knowledge is authored and
                      reviewed in the shared repo, not in consuming repos. There is no
                      bidirectional editing that causes submodule pointer merge conflicts.
                    - **Shallow.** One submodule at a known, predictable location. Not nested
                      or deeply embedded in the dependency tree.
                    - **Deliberately updated.** Consuming repos bump the submodule pointer when
                      they are ready for new knowledge. The pointer bump shows up as a clear
                      diff in the consuming repo's history.
                }

                Workflow:

                {[#]
                    1. Author or update knowledge in the shared repo
                    2. PR, review, merge — normal git flow with conflict resolution
                    3. In consuming repos, \`git submodule update --remote\` to pull the latest
                    4. Commit the pointer bump — the diff shows exactly what changed
                }

                This is especially important during early development when knowledge files
                are changing frequently and need review cycles. The git workflow is already
                built for this — no need to invent a new distribution mechanism.
            }

            # Alternatives @alternative-distribution
            {
                Other distribution mechanisms suit different situations:

                {[.]
                    - **Global directory** — \`~/lexica/\` for personal or machine-wide
                      knowledge. Good for individual developers or language-level knowledge
                      that does not need to be repo-pinned.
                    - **Package install** — knowledge distributed via npm, pip, or similar.
                      Makes sense for large ecosystems with versioning requirements, but adds
                      a publish pipeline for what are text files.
                    - **Remote MCP server** — shared knowledge served by a central server.
                      Good for large organisations with many repos and a platform team. Adds
                      network dependency and server maintenance.
                }
            }

            The system is distribution-agnostic. A knowledge file works the same regardless
            of how it arrived on disk. The tooling only cares that files exist in \`lexica/\`
            directories.
        }

        # What Goes Where — Scoping Guidelines @scoping-guidelines
        {
            {[table]
                Scope | Put it here if... | Examples
                Global | It applies to any project using this language/tool | C++ smart pointers, Python async patterns, git branching strategies
                Repo | It describes this project's architecture or conventions | "Our API uses REST with these conventions", "Error codes and their meanings", project roadmap
                Directory | It is specific to this module or subsystem | "The auth module uses OAuth2 with PKCE", "The renderer's frame pipeline stages", auth API spec
            }

            Rules of thumb:

            {[.]
                - If you would put it in a language textbook, it is global.
                - If you would put it in the project's onboarding guide, it is repo-level.
                - If only developers working in this directory need it, it is directory-level.
                - When in doubt, go one level broader — it is easier to find knowledge that is
                  too high than knowledge that is buried too deep.
                - Skills (how to) use \`type: skill\`. Everything else (specs, plans, status,
                  design notes, architecture) uses \`type: doc\`.
            }
        }
    }

    # Progressive Disclosure via Tooling @progressive-disclosure
    {
        The agent navigates through three levels, each served by an MCP tool or
        equivalent. Each level is small and focused.

        {[table]
            Level | What the agent sees | Approx tokens | MCP tool
            1. Discovery | UUID + \@about text for matching files | ~200 for 50 files | search / list_knowledge
            2. Orientation | Section headings with IDs from one file | ~50-100 | get_sections
            3. Detail | One section extracted by ID | ~200-1000 | get_section
        }

        Example workflow for an agent debugging a memory issue in \`src/auth/session.cpp\`:

        {[#]
            1. \`search("memory management C++")\` — scoped to \`src/auth/\`
            {
                Returns knowledge from auth, src, repo, and global levels. The global
                \`cpp-memory.sdoc\` skill and the repo-level \`error-handling.sdoc\` skill
                (which covers RAII patterns in this codebase) both appear. The auth-level
                \`session-lifecycle.sdoc\` doc also appears. ~200 tokens.
            }
            2. \`get_sections(uuid)\` — for the global C++ memory skill
            {
                Returns: RAII Pattern (\`raii-pattern\`), Smart Pointers (\`smart-pointers\`),
                Custom Allocators (\`custom-allocators\`). IDs are derived from heading text
                or explicit \@ids if present. ~50 tokens.
            }
            3. \`get_section(uuid, "smart-pointers")\` — loads one section
            {
                Full detail on smart pointer types and selection. ~500 tokens.
            }
        }

        Total: ~750 tokens, fully targeted. The agent saw what was available at all scope
        levels, chose the most relevant file, and loaded only the section it needed. No
        context pollution.
    }

    # MCP Server Interface @mcp-interface
    {
        Four tools, each scope-aware and returning a small focused payload:

        {[table]
            Tool | Input | Returns
            list_knowledge | working directory (optional) | UUID, path, scope, type (skill/doc), \@about text for all visible knowledge
            search | query + working directory (optional) | Ranked subset of visible knowledge matching the query, proximity-weighted
            get_sections | uuid or path | List of section headings with IDs (explicit \@id or derived) and first-line previews
            get_section | uuid or path + section ID | Full content of one section extracted from the AST
        }

        {[.]
            - \`list_knowledge\` and \`search\` use the working directory to determine scope.
              If omitted, they default to the repo root (showing repo + global knowledge).
            - Results include a \`scope\` field: "global", "repo", or a relative path like
              "src/auth". They also include the file's \`type\` ("skill" or "doc") from \@meta.
            - \`search\` ranks results by relevance to the query *and* proximity to the
              working directory. A local auth skill about OAuth ranks above a global
              security skill when working in the auth module.
            - \`get_sections\` returns derived IDs for all headings (from slugified heading
              text) and explicit \@ids where present. Both can be used with \`get_section\`.
            - \`get_section\` accepts either an explicit \@id or a derived ID.
            - If a file has no \@about section, \`list_knowledge\` and \`search\` generate one
              on the fly from the content (cached for performance).
        }

        The agent decides what to load next at each step. This is fundamentally better
        than RAG for this use case because the agent has *agency* in navigation rather
        than depending on embedding similarity to guess what is relevant. The scope
        awareness means the agent automatically sees the most contextually appropriate
        knowledge without explicit configuration.
    }

    # Design Decisions @decisions
    {
        # UUIDs for Stable Identity @uuid-decision
        {
            UUIDs are preferred over human-readable slugs. Files move, get renamed, repos
            get restructured. A UUID means cross-references survive all of that. The cost
            (opacity) is low because humans navigate by title and \@about text, and agents
            navigate via the manifest. The UUID is plumbing, not interface.

            UUIDs are always auto-generated — never typed by a human. Generation points:
            {[.]
                - **VS Code command** — "New Knowledge File" creates the file with UUID
                - **Pre-commit hook** — injects UUID into any knowledge file missing one
                - **CI / merge gate** — generates UUID if still missing at merge time
            }
        }

        # Tags Are Optional and AI-Generated @tags-optional
        {
            Tags in \@meta (e.g., \`tags: cpp, memory, raii\`) are optional. The \@about
            text is the primary discovery mechanism — modern LLMs do semantic matching well
            enough that explicit tags provide marginal improvement.

            When tags are desired, they should be AI-generated from the content at commit
            or merge time, alongside the \@about paragraph. The same AI pass that generates
            the \@about can suggest tags. The author or reviewer can accept, modify, or
            remove them.

            If a team wants tag discipline, a \`taxonomy.sdoc\` at the repo root can
            define canonical tags, and the AI generator can be constrained to that
            vocabulary.
        }

        # One "About" Section, Not Description + Summary @single-about
        {
            The initial design had separate Description and Summary sections. In practice
            this distinction collapses — authors write the same thing in both, or let them
            drift apart. A single \@about section answers the one question that matters for
            discovery: "Is this file relevant to my current task?"

            The \@about section is the highest-value piece of metadata and the one most
            suited to AI generation. See \@about-generation for the generation strategy.
        }

        # No Hand-Maintained Index @no-manual-index
        {
            The initial design had an Index section listing all content sections. This is
            duplicated effort that will drift as sections are added, removed, or renamed.

            The tooling generates the section list by parsing the AST and extracting
            headings — using explicit \@ids where present, and derived IDs (slugified
            heading text) elsewhere. This is always accurate and costs the author nothing.
        }

        # Discovery Model @discovery-model
        {
            Discovery is simple: tooling looks for \`lexica/\` directories. No metadata
            declaration is needed, no hidden directories, no special file naming.

            {[.]
                - **Visible** — \`lexica/\` is a plain directory that shows up in file
                  explorers, \`ls\`, and IDE sidebars. Developers and agents can browse
                  it directly.
                - **Format-agnostic** — any file in a \`lexica/\` directory is knowledge.
                  SDOC files get the richest tooling (unambiguous section extraction), but
                  Markdown or plain text files are discovered and served too.
                - **Zero configuration** — no declarations, no manifest registration, no
                  metadata required. Put a file in \`lexica/\`, it is knowledge.
                - **Familiar** — \`lexica/\`, \`tests/\`, \`src/\` — developers already
                  know what a named directory means.
                - **No collisions** — unlike \`docs/\`, the name \`lexica/\` is not used by
                  any existing convention (GitHub Pages, user-facing docs, etc.).
            }
        }

        # Cross-File References @cross-references
        {
            Knowledge files can reference other files to express prerequisites or related
            knowledge. This creates a navigable knowledge graph.

            Cross-file reference resolution is a tooling concern. The MCP server resolves
            references and offers the agent the referenced file's \@about text, without
            loading the full file.

            The exact syntax is an open question. Options:
            {[.]
                - **Inline UUID:** \`@550e8400-e29b-41d4-a716-446655440000\` — unambiguous
                  but unreadable in source
                - **Registered alias:** \`@cpp-basics\` resolved via the manifest — readable
                  but requires the manifest to be available
                - **Path-like reference:** \`@global/cpp-basics\` or \`@repo/architecture\` —
                  readable and encodes scope, but couples to file organisation
            }

            A practical default: use the file's \@id from its root heading (e.g.,
            \`@cpp-memory\`) as the cross-reference alias, with the manifest providing
            the UUID-to-path resolution. This is readable, stable if IDs are maintained,
            and consistent with existing SDOC \@id usage.
        }

        # Manifest Auto-Generation @auto-manifest
        {
            A script walks the directory tree, collects knowledge files from \`lexica/\`
            directories at each level, parses their metadata, and emits a manifest. This
            is the master index.

            {[.]
                - **Generated, not hand-maintained.** Running the generator always produces
                  an accurate manifest. It can be run as a pre-commit hook, CI step, or
                  on-demand by the MCP server.
                - **Scope-aware.** The manifest records which scope level each file belongs
                  to (global, repo, or the relative directory path) and its \`type\` (skill
                  or doc) from \@meta.
                - **Output formats:** JSON for programmatic consumption (MCP server, CI
                  tools), optionally also an SDOC file for human browsing.
                - **Committed or generated on the fly.** Teams can commit the manifest for
                  offline use and fast startup, or generate it on demand. The MCP server
                  should support both modes.
                - **Generates missing metadata.** If a knowledge file has no \@about, the
                  manifest generator can invoke an AI agent to produce one from the content
                  and either inject it into the file or store it in the manifest only.
            }
        }
    }

    # Merge Gate @merge-gate
    {
        The merge gate is *generative*, not just validating. Rather than blocking merges
        for missing metadata, it fills in what is missing:

        {[#]
            1. **UUID check** — if a knowledge file has no uuid in \@meta, generate one and
               add it to the file
            2. **\@about check** — if missing or if content sections have changed substantially,
               invoke an AI agent to generate or update the \@about paragraph
            3. **Tags check** (optional, team opt-in) — if the team uses tags, the AI agent
               suggests tags from content and the taxonomy
            4. **UUID uniqueness** — verify no two knowledge files share a UUID
            5. **Section ID coverage** (warning only) — note any content headings without
               explicit \@ids, for teams that want comprehensive cross-referencing
        }

        The generated metadata appears in the PR diff for review. The author can accept it
        as-is, tweak it, or override it. The gate should feel like a helpful assistant, not
        a bureaucracy.

        For teams that prefer a lighter touch, the gate can be validation-only (just check
        that UUID and \@about exist, without generating). The generative mode is recommended
        because it removes friction entirely — the author never has to think about metadata.
    }

    # Red Team Summary @red-team
    {
        Critiques considered and how the design responds:

        # "Metadata rots — DITA/Javadoc proved this" @critique-rot
        {
            Addressed by making metadata AI-generated rather than human-maintained. The
            author writes content; tooling generates UUID, \@about, tags, and section
            indices. The only thing that can rot is the \@about text, and the merge gate
            regenerates it when content changes.
        }

        # "Agents will just load everything — tokens are cheap" @critique-tokens
        {
            Context windows are growing but codebases grow faster. More critically, the
            "lost in the middle" problem means more context often *degrades* agent
            performance. Targeted loading is not just cheaper — it produces better results.
        }

        # "RAG will solve this generically" @critique-rag
        {
            RAG is passive retrieval — the agent gets what the embeddings suggest. This
            approach gives agents *agency* in navigation. The agent reasons about what
            to load, which is more powerful for structured knowledge. RAG also loses
            structural context (what else is in the file, what are the prerequisites).
            Both approaches can coexist — RAG as a fallback, structured navigation as
            the primary path.
        }

        # "Nobody will write the knowledge files" @critique-adoption
        {
            Two concerns here: metadata and content. Metadata is a non-issue — the author
            writes content, tooling generates everything else. The metadata authoring
            burden is zero.

            The content authoring burden is real but unavoidable — someone has to write
            the knowledge. The system makes this as frictionless as possible (skeleton
            generators, AI-assisted \@about, format-agnostic discovery) and ensures that
            whatever gets written is immediately useful (progressive disclosure, scope-aware
            search). The bet is that lowering friction enough will cross the threshold
            where writing a knowledge file is less work than repeatedly explaining the
            same thing to agents.
        }

        # "SDOC is niche — use Markdown" @critique-format
        {
            The discovery mechanism is format-agnostic — any file in a \`lexica/\`
            directory is knowledge, regardless of format. SDOC provides the
            richest tooling (unambiguous section extraction via braces, \@meta, \@id
            references), but Markdown files are first-class citizens in discovery and
            can be served to agents with best-effort section parsing. Teams that prefer
            Markdown can use it. Teams that want precise progressive disclosure use SDOC.
        }

        # "The granularity problem has no answer" @critique-granularity
        {
            True — there is no universal answer. The convention should be: one file per
            topic that an expert would think of as a coherent unit. Too broad (all of C++)
            makes the \@about text useless. Too narrow (one function) makes navigation
            overhead dominate. Domain teams decide granularity; the format does not
            enforce it.
        }

        # "Shared repos via submodules are painful" @critique-submodules
        {
            The usual submodule pain points — bidirectional editing, merge conflicts on
            the pointer, developers forgetting to init — do not apply to this use case.
            Shared knowledge is read-only from the consuming repo's perspective: authored
            and reviewed upstream, pulled deliberately downstream. The submodule is
            shallow (one, at a known location) and updates are explicit. This is actually
            one of the use cases submodules were designed for. See \@submodule-distribution
            for the recommended workflow.
        }

        # "Why not two directories — skills/ and docs/?" @critique-two-dirs
        {
            The skill vs doc distinction is real and valuable — "How do I do this?" and
            "What is this?" are different questions that lead to different content. But
            this distinction belongs in metadata (\`type: skill\` or \`type: doc\` in \@meta),
            not in the filesystem.

            A single \`lexica/\` directory avoids several problems:
            {[.]
                - **No \`docs/\` collision.** Many projects already use \`docs/\` for GitHub
                  Pages or user-facing documentation. A separate name eliminates ambiguity.
                - **Simpler discovery.** Tooling checks for one directory name instead of
                  two at each level of the tree.
                - **Colocation.** Related skills and docs about the same module live
                  together in one place rather than being split across two sibling
                  directories.
                - **The type is still visible.** The \`type\` field in \@meta (and in search
                  results) tells the agent whether a file is prescriptive or descriptive
                  before opening it.
            }
        }

        # "Agents won't follow the progressive disclosure protocol" @critique-agent-discipline
        {
            Agents are impatient. Given the option, many will try to load an entire file
            rather than navigating section by section. The progressive disclosure model
            only works if agents actually use it.

            Mitigation: the MCP tools *are* the interface. There is no "load the whole
            file" tool. The agent must go through \`list_knowledge\` → \`get_sections\` →
            \`get_section\`. The tools enforce the protocol by design.

            Additionally, the skill that teaches agents about Lexica (the SDOC skill
            reference) must explicitly instruct agents on the navigation pattern and
            when to stop reading. The tools enforce boundaries; the instructions guide
            behaviour within those boundaries.
        }

        # "\@about quality will be inconsistent" @critique-about-quality
        {
            If AI-generated \@about paragraphs are wrong or misleading, agents make bad
            navigation decisions. The whole progressive disclosure model's effectiveness
            depends on \@about quality.

            Mitigation: the merge gate regenerates \@about when content changes, so drift
            is caught at PR review time. The \@about appears in the PR diff — reviewers
            can see and correct it. And a bad summary that gets an agent 80% of the way
            is still far better than no summary at all, which gives the agent 0%
            information for navigation.
        }
    }

    # Scope of This Document @doc-scope
    {
        This document covers the design of **Lexica** — how knowledge files are structured,
        discovered, scoped, and navigated by AI agents and developers.

        Related but deliberately out of scope:

        {[.]
            - **Agent orientation protocol** — the broader question of how an AI agent comes
              up to speed at any node of a codebase, considering not just skills and docs
              but also code structure, test results, and project state. The progressive
              disclosure mechanism described here is one component of that larger problem.
            - **SDOC format specification** — the syntax and parsing rules for SDOC are
              defined in the SDOC Guide. This document assumes familiarity with SDOC and
              describes how existing SDOC features are applied to the Lexica use case.
        }
    }

    # Open Questions @open-questions
    {
        {[.]
            - Cross-file reference syntax: heading \@id alias vs UUID vs path convention
            - How to handle prerequisites / dependency ordering in the knowledge graph
            - Whether the manifest should be committed (for offline use) or only generated
            - How to version knowledge content as it evolves (per-file version in \@meta?)
            - Integration with existing Anthropic skills folder conventions (\`.claude/\`)
            - How the MCP server discovers the global knowledge directory (\`~/lexica/\`
              vs an environment variable vs MCP server configuration)
            - Whether knowledge files should support an \`obsoletes\` field in \@meta for
              graceful deprecation and migration
            - Which AI model/service to use for \@about generation (local vs API, cost
              considerations for CI pipelines)
            - Whether derived section IDs should be returned alongside explicit \@ids or
              only when no explicit \@id exists
            - Best-effort section parsing strategy for non-SDOC files (Markdown heading
              heuristics, plain text chunking)
            - Whether the \`type\` field in \@meta should have a default value (e.g., "doc")
              when omitted, or whether tooling should require it explicitly
        }
    }
}
